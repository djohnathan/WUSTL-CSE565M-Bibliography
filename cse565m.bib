@inproceedings{Ye2022,
    author = {Ye, Hanchen and Hao, Cong and Cheng, Jianyi and Jeong, Hyunmin and
              Huang, Jack and Neuendorffer, Stephen and Chen, Deming},
    booktitle = {2022 IEEE International Symposium on High-Performance Computer
                 Architecture (HPCA)},
    title = {ScaleHLS: A New Scalable High-Level Synthesis Framework on
             Multi-Level Intermediate Representation},
    year = {2022},
    pages = {741-755},
    creationdate = {2024-06-08T08:44:18},
    doi = {10.1109/HPCA53966.2022.00060},
    groups = {cse565m},
    keywords = {Productivity;Codes;Estimation;Transforms;Machine
                learning;Libraries;Hardware;High-Level
                Synthesis;MLIR;Compiler;FPGA;Optimization;Design Space
                Exploration,cse565m},
    modificationdate = {2024-06-08T08:51:42},
    owner = {Anthony Cabrera, Star Student},
}

@inproceedings{Zohouri2016,
    author = {Zohouri, Hamid Reza and Maruyama, Naoya and Smith, Aaron and
              Matsuda, Motohiko and Matsuoka, Satoshi},
    booktitle = {SC '16: Proceedings of the International Conference for High
                 Performance Computing, Networking, Storage and Analysis},
    title = {Evaluating and Optimizing OpenCL Kernels for High Performance
             Computing with FPGAs},
    year = {2016},
    month = {Nov},
    pages = {409-420},
    abstract = {We evaluate the power and performance of the Rodinia benchmark
                suite using the Altera SDK for OpenCL targeting a Stratix V FPGA
                against a modern CPU and GPU. We study multiple OpenCL kernels
                per benchmark, ranging from direct ports of the original GPU
                implementations to loop-pipelined kernels specifically optimized
                for FPGAs. Based on our results, we find that even though OpenCL
                is functionally portable across devices, direct ports of
                GPU-optimized code do not perform well compared to kernels
                optimized with FPGA-specific techniques such as sliding windows.
                However, by exploiting FPGA-specific optimizations, it is
                possible to achieve up to 3.4x better power efficiency using an
                Altera Stratix V FPGA in comparison to an NVIDIA K20c GPU, and
                better run time and power efficiency in comparison to CPU. We
                also present preliminary results for Arria 10, which, due to
                hardened FPUs, exhibits noticeably better performance compared to
                Stratix V in floating-point-intensive benchmarks.},
    creationdate = {2024-06-08T08:53:38},
    doi = {10.1109/SC.2016.34},
    groups = {cse565m},
    issn = {2167-4337},
    keywords = {Field programmable gate arrays;Kernel;Optimization;Benchmark
                testing;Programming;Graphics processing units;Performance
                evaluation;FPGA;Performance evaluation;OpenCL;Heterogeneous
                computing},
    modificationdate = {2024-06-08T08:53:38},
    owner = {Anthony Cabrera},
}

@article{zhao2024hlperf,
    title = {HLPerf: Demystifying the Performance of HLS-based Graph Neural
             Networks with Dataflow Architectures},
    author = {Zhao, Chenfeng and Faber, Clayton J and Chamberlain, Roger D and
              Zhang, Xuan},
    journal = {ACM Transactions on Reconfigurable Technology and Systems},
    year = {2024},
}

@article{sohrabizadeh2022autodse,
    title = {AutoDSE: Enabling software programmers to design efficient FPGA
             accelerators},
    author = {Sohrabizadeh, Atefeh and Yu, Cody Hao and Gao, Min and Cong, Jason
              },
    journal = {ACM Transactions on Design Automation of Electronic Systems
               (TODAES)},
    volume = {27},
    number = {4},
    pages = {1--27},
    year = {2022},
    publisher = {ACM New York, NY},
}

@article{Kastner2018,
    author = {{Kastner}, R. and {Matai}, J. and {Neuendorffer}, S.},
    journal = {ArXiv e-prints},
    title = {{Parallel Programming for FPGAs}},
    year = {2018},
    month = May,
    archiveprefix = {arXiv},
    eprint = {1805.03648},
    groups = {cse565m},
    keywords = {Computer Science - Hardware Architecture},
}

@article{SAHEBI2025107497,
    title = {HashGrid: An optimized architecture for accelerating graph
             computing on FPGAs},
    journal = {Future Generation Computer Systems},
    volume = {162},
    pages = {107497},
    year = {2025},
    issn = {0167-739X},
    doi = {https://doi.org/10.1016/j.future.2024.107497},
    url = {https://www.sciencedirect.com/science/article/pii/S0167739X24004618},
    author = {Amin Sahebi and Marco Procaccini and Roberto Giorgi},
    keywords = {Big graph computing, High-performance computing, FPGA design,
                Large-scale graph, Graph partitioning},
    abstract = {Large-scale graph processing poses challenges due to its size
                and irregular memory access patterns, causing performance
                degradation in common architectures, such as CPUs and GPUs.
                Recent research includes accelerating graph processing using
                Field Programmable Gate Arrays (FPGAs). FPGAs can provide very
                efficient acceleration thanks to reconfigurable on-chip
                resources. Although limited, these resources offer a larger
                design space than CPUs and GPUs. We propose an approach in which
                data are preprocessed in small chunks with an optimized graph
                partitioning technique for execution on FPGA accelerators. The
                chunks, located on the host, are streamed directly into a
                customized memory layer implemented in the FPGA, which is tightly
                coupled with the processing elements responsible for the graph
                algorithm execution. This improves application memory access
                latency, which is crucial in large-sale graph computing
                performance. This work presents a hardware design that, combined
                with graph partitioning, enables us to achieve high-performance
                and potentially scalable handling of large graphs (i.e., graphs
                with millions of vertices and billions of edges in current
                scenarios) while using popular graph algorithms. The proposed
                framework accelerates performance 56 times compared with CPU
                (multicore with 16 logical cores in our reference experiments),
                2.5 times and 4 times faster compared to state-of-the-art FPGA
                and GPU solutions (FPGA has 15 compute units, and GPU reference
                has 128 streaming-multiprocessors in our experiments),
                respectively, when using the PageRank algorithm. For the
                Single-Source-Shortest-Past (SSSP) algorithm, we achieve speedups
                of up to 65x, 26x, and 18x compared to CPU, GPU, and FPGA works,
                respectively. Lastly, in the context of the Weakly Connected
                Component (WCC) algorithm, our framework achieves a speedup of up
                to 403 times compared to the CPU, 7.4x against the GPU, and it is
                faster than the FPGA alternatives up to 10.3x.},
}


@INPROCEEDINGS{9221526,
  author={Singh, Gagandeep and Diamantopoulos, Dionysios and Hagleitner, Christoph and Gomez-Luna, Juan and Stuijk, Sander and Mutlu, Onur and Corporaal, Henk},
  booktitle={2020 30th International Conference on Field-Programmable Logic and Applications (FPL)}, 
  title={NERO: A Near High-Bandwidth Memory Stencil Accelerator for Weather Prediction Modeling}, 
  year={2020},
  volume={},
  number={},
  pages={9-17},
  keywords={Energy consumption;Multithreading;Weather forecasting;Predictive models;Energy efficiency;Climate change},
  doi={10.1109/FPL50879.2020.00014}}

@INPROCEEDINGS{9255725,
  author={Luthra, Siddhant and Khalid, Mohammed A.S. and Moin Oninda, Mohammad Abdul},
  booktitle={2020 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)}, 
  title={FPGA-Based Evaluation and Implementation of an Automotive RADAR Signal Processing System using High-Level Synthesis}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  keywords={Hardware;Field programmable gate arrays;Tools;Radar;Automotive engineering;Radar signal processing;Design methodology;High-Level Synthesis (HLS);Hardware Description Language (HDL);Automotive Radar Signal Processing System;Register Transfer Level (RTL);Xilinx Vivado HLS;Quality-of-Result (QoR)},
  doi={10.1109/CCECE47787.2020.9255725}}

@INPROCEEDINGS{djohnathanFS24,
    owner = {Johnathan Dunker},
    author = {Ye, Wenhua and Zhou, Xu and Zhou, Joey and Chen, Cen and Li, Kenli},
    title = {Accelerating Attention Mechanism on FPGAs based on Efficient Reconfigurable Systolic Array},
    year = {2023},
    issue_date = {November 2023},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {22},
    number = {6},
    issn = {1539-9087},
    url = {https://doi.org/10.1145/3549937},
    doi = {10.1145/3549937},
    abstract = {Transformer model architectures have recently received great interest in natural language, machine translation, and computer vision, where attention mechanisms are their building blocks. However, the attention mechanism is expensive because of its intensive matrix computations and complicated data flow. The existing hardware architecture has some disadvantages for the computing structure of attention, such as inflexibility and low efficiency. Most of the existing papers accelerate attention by reducing the amount of computation through various pruning algorithms, which will affect the results to a certain extent with different sparsity. This paper proposes the hardware accelerator for the multi-head attention (MHA) on field-programmable gate arrays (FPGAs) with reconfigurable architecture, efficient systolic array, and hardware-friendly radix-2 softmax. We propose a novel method called Four inputs Processing Element (FPE) to double the computation rate of the data-aware systolic array (SA) and make it efficient and load balance. Especially, the computation framework is well designed to ensure the utilization of SA efficiently. Our design is evaluated on a Xilinx Alveo U250 card, and the proposed architecture achieves 51.3\texttimes{}, 17.3\texttimes{} improvement in latency, and 54.4\texttimes{}, 17.9\texttimes{} energy savings compared to CPU and GPU.},
    journal = {ACM Trans. Embed. Comput. Syst.},
    month = nov,
    articleno = {93},
    numpages = {22},
    keywords = {Transformer, attention, FPGA, reconfigurable systolic array, softmax, Accelerator}
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:cse565m\;0\;1\;\;\;\;;
}

